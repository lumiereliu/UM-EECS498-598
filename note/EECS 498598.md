# EECS 498/598

## 02 Image Classification

1. K-Nearest Neighbor

   - K: Hyper Parameter

     - too low: overfitting
     - too high: useless
     - Cross Validation Data set: Split data into folds![](D:\CSDIY\EECS498\note\cross-validation.png)

   - Curse of dimensionality

     - train O(1) but test O(N)

   - Distance metrics on pixels are not informative

     - L1(Manhattan) distance / L2(Euclidean) distance 
       $$
       d_1(I_1, I_2) = \Sigma_P|I_1^P-I_2^P|\qquad d_2(I_1, I_2) = \sqrt{\Sigma_P(I_1^P-I_2^P)^2}
       $$
       
     
   - KNN + ConvNet works well

## 03 Linear Classifiers

1. Interpreting a Linear Classifier

   - Algebraic Viewpoint:
     - Bias Trick![bias-trick-before](D:\CSDIY\EECS498\note\bias-trick-before.png)![bias-trick-after](D:\CSDIY\EECS498\note\bias-trick-after.png)
   - Visual Viewpoint:
     - A single template cannot capture multiple modes of the data
       - a horse head towards different directions
   - Geometric Viewpoint:
     - hard cases for a linear classifier: 1 <= L2 norm <= 2
     - cannot learn xor

2. Loss

   - Multiclass SVM Loss:

   $$
   L_i=\Sigma_{j\neq{y_i}}max(0,s_j-s_{y_i}+1)
   $$

   How should we choose between W and 2W if they both perform the same on the training data?

   - Regularization:
     $$
     R_1(W) = \Sigma_k\Sigma_l|W_{k,l}| \qquad R_2(W) = \Sigma_k\Sigma_lW_{k,l}^2
     $$

     - Express preferences
       - L1 Regularization put all of the weight on a single feature
       - L2 Regularization likes to spread out the weights
     - Avoid overfitting

   - Cross-Entropy Loss:
     $$
     L_i = -logP(Y=y_i|X=x_i)
     $$
     *SVM = 0 Cross-Entropy > 0 in some cases*

## 04 Optimization

- Gradients Descent

  - gradient check: Always use **analytic gradient**, but check implementation with **numerical gradient**

  - Batch Gradient Descent / Stochastic Gradient Descent (minibatch)

  - Hyperparameters:

    - Weight initialization
    - Number of steps
    - Learning rate
    - Batch size
    - Data sampling

  - SGD+Momentum: 逃离局部最小值

    ```python
    v = 0
    for t in range(num_steps):
    	dw = compute_gradient(w)
        #rho gives "friction"; typically rho = 0.9 or 0.99
    	v = rho * v + dw
    	w -= learning_rate * v
    ```

  - Nestrov Momentum: 收敛更快，提前瞄准目标

    ```python
    v = 0
    for t in range(num_steps):
        dw = compute_gradient(w)
        old_v = v
        v = rho * v - learning_rate * dw
        w -= rho * old_v - (1 + rho) * v
    ```

  - AdaGrad: 频繁/不常更新的参数学习率变小/变大

    ```python
    grad_squared = 0
    for t in range(num_steps):
        dw = compute_gradient(w)
        grad_squared += dw * dw
        w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)
    ```

  - RMSProp: "Leak Adagrad" 解决AdaGrad学习率递减太快的问题

    ```python
    grad_squared = 0
    for t in range(num_steps):
        dw = compute_gradient(w)
        grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dw * dw
        w -= learning_rate * dw / (grad_squared.sqrt() + 1e-7)
    ```

  - Adam(almost): RMSProp + Momentum

    ```python
    moment1 = 0
    moment2 = 0
    for t in range(num_steps):
        dw = compute_gradient(w)
        moment1 = beta1 * moment1 + (1 - beta1) * dw
        moment2 = beta2 * moment2 + (1 - beta2) * dw * dw
        w -= learning_rate * moment1 / (moment2.sqrt() + 1e-7)
    ```

    When t = 0, moment2 is very large, so

  - Bias correction:

    ```python
    moment1 = 0
    moment2 = 0
    for t in range(num_steps):
        dw = compute_gradient(w)
        moment1 = beta1 * moment1 + (1 - beta1) * dw
        moment2 = beta2 * moment2 + (1 - beta2) * dw * dw
        moment1_unbias = moment1 / (1 - beta1 ** t)
        moment2_unbias = moment2 / (1 - beta2 ** t)
        w -= learning_rate * moment1_unbias / (moment2_unbias.sqrt() + 1e-7)
    ```

    *Adam is a good default choice in many cases. SGD+Momentum can outperform Adam but may require more tuning*

    *If you can afford to do full batch updates then try out L-BFGS(and don't forget to disable all sources of noise)*

## 05 Neural Networks

- Feature transform + Linear classifier allows nonlinear decision boundaries

- Activation Function: Relu used mostly
- Space Warping: more hidden units = more capacity (more complicated decision boundary)
  - overfitting? don't regularize with size; instead use stronger L2
- Universal Approximation: Approximating a function f: R->R with a two-layer ReLU network
  - build a "bump function" using four hidden units
  - with 4K hidden units we can build a sum of K bumps
- Convex functions are easy to optimize: can derive theoretical guarantees about converging to global minimum

## 06 Backpropagation

- Flat gradient code / PyTorch Autograd Functions  

- Backprop with Vectors:

  - About relu: Jacobian is sparse: never explicitly form Jacobian; instead use implicit multiplication

- Backprop with Matrices:

  - Matrix Multiplication:
    $$
    dL/dx = (dL/dy)w^T \qquad dL/dw = x^T(dL/dy)
    $$

    - Easy way to remember: It's the only way the shapes work out

- Reverse-Mode Automatic Differentiation

- Forward-Mode Automatic Differentiation: beyond machine learning

- Backprop: Higher-Order Derivatives![higher-order derivatives](D:\CSDIY\EECS498\note\higher-order derivatives.png)

  *Example: Regularization to penalize the norm of the gradient*
  $$
  R(W) = ||\frac{\partial L}{\partial W}||_2^2
  $$

## 07 Convolutional Networks

- Convolution layers

  ![convolution-layers](D:\CSDIY\EECS498\note\convolution-layers.png)

  - the depth / channels must match the number of the filter

  - use ReLU to avoid stacking convolutions directly

  - **Problem**: Feature maps "shrink" with each layer

    - **Solution**: padding

      - Input: W

      - Filter: K

      - Padding: P

      - Output: W - K + 1 + 2P

        *very common: set P = (K - 1) / 2 to make output have same size as input*

  - Receptive Fields (2 interpretations)

    - kernel size K, receptive field size K * K 
    - Each successive convolution adds K - 1 to the receptive field size with L layers the receptive field size is 1 + L * (K - 1)

  - **Problem**: For large images we need many layers for each output to see the whole image

    - **Solution**: Stride

      - Input: W

      - Filter: K

      - Padding: P

      - Stride: S

      - Output: (W- K + 2P) / S + 1

        *1 by 1 filter usually as an adapter*

    ![convolution-summary](D:\CSDIY\EECS498\note\convolution-summary.png)

- Pooling layers: involve no learnable parameters

  - Max Pooling: introduces invariance to small spatial shifts

  - Average Pooling

    ![image-20250117213858093](C:\Users\lumiere\AppData\Roaming\Typora\typora-user-images\image-20250117213858093.png)

- Batch Normalizaiton
  $$
  \widehat{x}^{(k)} = \frac{x^{(k)}-E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}}
  $$
  *var add epsilon to avoid divide zero*

  - **Problem**: What if zero-mean, unit variance is too hard of a constraint
    - learning $\gamma$ = $\sigma$, $\beta$ = $\mu$ will recover the identity function
      $$
      y_{i,j} = \gamma_j\widehat{x}_{i,j} + \beta_j
      $$
    
  - **Problem**: Estimates depend on minibatch; can't do this at test-time
  
    - $\mu_j$ and $\sigma_j^2$ = average of values seen during training
  
    - can be fused with the previous fully-connected or conv layer
      $$
      y = \gamma(x-\mu) / \sigma + \beta
      $$
      *batch normalizaiton behaves differently during training and testing: this is a very common source of bugs*
  
- Layer Normalization / Instance Normalization: change dimension view![normalization](D:\CSDIY\EECS498\note\normalization.png)

## 08 CNN Architectures

- AlexNet: memory / params / flop![AlexNet](D:\CSDIY\EECS498\note\AlexNet.png)
- VGG:![VGG](D:\CSDIY\EECS498\note\VGG.png)
  - All conv are 3 by 3 stride 1 pad 1
  - All max pool are 2 by 2 stride 2
  - After pool, double channels: Conv layers at each spatial resolution take the same amount of computation
- GoogleNet: ![GoogleNet](D:\CSDIY\EECS498\note\GoogleNet.png)
  - Stem network at the start aggressively downsamples input
  - Inception Module: local unit with parallel branches
    - uses 1 by 1 "Bottleneck" layers to reduce channel dimension before expensive conv
  - Global Average Pooling
    - no large FC layers at the end
    - use average pooling to replace flatten and one linear layer to produce class scores
  - Auxiliary Classifiers
- ResNet:![ResNet](D:\CSDIY\EECS498\note\ResNet.png)
  - idea: the deep model seems to be underfitting since it also performs worse than the shallow model on the training set
  - A deeper model can emulate a shallower model: copy layers from shallower model, set extra layers to identity
  - Bottleneck Block![ResNet-Bottleneck](D:\CSDIY\EECS498\note\ResNet-Bottleneck.png)
  - ResNext: inception + ResNet
  - MobileNets: tiny networks

## 09 Hardware and Software

- Deep Learning Hardware

  - CPU / GPU / TPU

  - mix precision: 16-bit / 32-bit float

  - for deep learning, GPU memory is important

- Deep Learning Software

  - two mainstream frameworks: PyTorch and TensorFlow

  - PyTorch

    - Tensor: basic operation

    - Autograd

      - requires_grad = True

      - if there are some functions like

        ```python
        def sigmoid(x):
            return 1.0 / (1.0 + (-x).exp())
        ```

        loss.backward() will produce a giant computational grath: numerically unstable

        Solution: define new autograd operators by subclassing Function, define forward and backward

    - nn

      ```python
      import torch
      
      N, D_in, H, D_out = 64, 1000, 100, 10
      x = torch.randn(N, D_in)
      y = torch.randn(N, D_out)
      
      model = torch.nn.Sequential(
      			torch.nn.Linear(D_in, H),
      			torch.nn.ReLU(),
      			torch.nn.Linear(H, D_out))
      
      learning_rate = 1e-2
      for t in range(500):
          y_pred = model(x)
          loss = torch.nn.functional.mse_loss(y_pred, y)
          
          loss.backward()
          
          with torch.no_grad():
              for param in model.parameters():
                  param -= learning_rate * param.grad
          model.zero_grad()
      ```

    - optim

      ```python
      import torch
      
      N, D_in, H, D_out = 64, 1000, 100, 10
      x = torch.randn(N, D_in)
      y = torch.randn(N, D_out)
      
      model = torch.nn.Sequential(
      			torch.nn.Linear(D_in, H),
      			torch.nn.ReLU(),
      			torch.nn.Linear(H, D_out))
      
      learning_rate = 1e-4
      optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)
      
      for t in range(500):
          y_pred = model(x)
          loss = torch.nn.functional.mse_loss(y_pred, y)
          
          loss.backward()
          
          optimizer.step()
          optimizer.zero_grad()
      ```

    - module

      ```python
      class ParallelBlock(torch.nn.Module):
          def __init__(self, D_in, H, D_out):
              super(TwoLayerNet, self).__init__()
              self.linear1 = torch.nn.Linear(D_in, D_out)
              self.linear2 = torch.nn.Linear(D_in, D_out)
          def forward(self, x):
              h1 = self.linear1(x)
              h2 = self.linear2(x)
              return (h1 * h2).clamp(min = 0)
          
      model = torch.nn.Sequentail(
      			ParallelBlock(D_in, H),
          		ParallelBlock(H, H),
          		torch.nn.Linear(H, D_out))
      ```

    - DataLoader / Pretrained Models

      ```python
      loader = DataLoader(TensorDataset(x, y), batch_size = 8)
      
      resnet101 = torchvision.models.resnet101(pretrained = True)
      ```

    - Dynamic Computation Graphs

      ```python
      for t in range(500):
          w2 = w2a if prev_loss < 5.0 else w2b
      ```

    - Static Graphs with JIT: optimize the graph for you before it runs / Serialize: train model in Python, deploy in C++

      ```python
      @torch.jit.script
      def model(x, y, w1, w2a, w2b, prev_loss):
          w2 = w2a if prev_loss < 5.0 else w2b
          y_pred = x.mm(w1).clamp(min = 0).mm(w2)
          loss = (y_pred - y).pow(2).sum()
          return loss
      ```


## 10-11 Training Neural Networks

1. One time setup

   - Activation Functions

     - Sigmoid

       - have nice interpretation as a saturating "firing rate" of a neuron
       - 3 Problems:
         - Saturated neurons "kill" the gradients
         - Sigmoid outputs are not zero-centered: gradients on **w** always all positive or all negative
         -  exp() is a bit compute expensive 

     - Tanh

       - zero centered
       - still kills gradients when saturated

     - ReLU

       - does not saturate
       - very computationally efficient
       - converges much faster than sigmoid / tanh in practcice
       - not zero-centered output
       - will "die": gradient = 0

     - Leaky ReLU: 
       $$
       f(x) = max(0.01x, x)
       $$

       - will not "die"

     - Parametric Rectifier(PReLU): 
       $$
       f(x) = max(\alpha x, x)
       $$

       - backprop into $\alpha$

     - Exponential Linear Unit(ELU)
       $$
       f(x) = \begin{cases}
       x &x > 0 \\
       \alpha(exp(x) - 1) &x\leq0
       \end{cases}
       $$

       - Negative saturation regime compared with Leaky ReLU adds some robustness to noise

     - Scaled Exponential Linear Unit(SELU)
       $$
       selu(x) = \begin{cases}
       \lambda x & x < 0 \\
       \lambda(\alpha e^x - \alpha) &otherwise \\
       \end{cases}
       $$

       - "Self-Normalizing" property; can train deep SELU networks without BatchNorm

     - Summary

       - Don't think too hard. Just use ReLU
       - Try out Leaky ReLU / ELU / SELU / GELU if you need to squeeze that last 0.1%
       - Don't use sigmoid or tanh

   - Data Preprocessing

     - Decorrelation: rotate the data cloud

     - Whitening: Normalization after decorrelation

     - after normalization: less sensitive to small changes in weights; easier to optimize

     - for Images

       - substract the mean image(AlexNet)

       - subtract per-channel mean(VGGNet)

       - subtract per-channel mean and divide by per-channel std

         *not common to do PCA or whitening*

   - Weight Initialization

     - Activation Statistics

       - too low: all activations tend to zero for deeper network layers
       - too high: all activations saturate
       - local gradients all zero, no learning

     - Xavier Initialization

       ```python
       dims = [4096] * 7  # 7层网络，每层4096个神经元
       for Din, Dout in zip(dims[:-1], dims[1:]):
           # 权重初始化：随机数/sqrt(输入维度)
           W = np.random.randn(Din, Dout) / np.sqrt(Din)
           x = np.tanh(x.dot(W))  # 使用tanh激活函数
           hs.append(x)
       ```

       *don't fit ReLU*

     - Kaiming Initialization

       - ReLU correction: std = sqrt(2 / Din)

       - If we initialize with MSRA: then Var(F(x)) = Var(x) But then Var(F(x) + x) > Var(x)

         Solution: Initialize first conv with MSRA, initialize second conv to zero.

   - Regularization

     - L1 / L2 Regularization

     - Dropout: In each forward pass, randomly set some neurons to zero

       - Probability of dropping is a hyperparameter; 0.5 is common

       - Interpretation:

         - Forces the network to have a redundant representation; Prevents co-adaptation of features
         - Dropout is training a large ensemble of models

       - **Problem**: Dropout makes our output and test time random

         **Solution**: drop in forward pass and scale at test time

     - Batch Normalization

       *Later architectures use global average pooling instead of fully-connected layers: they don't use dropout at all*

     - Data Augmentation

       - Random Crops and Scales
       - Horizontal Flips
       - Color Jitter

     - DropConnect / Fractional Max Pooling / Stochastic Depth / Cutout / Mixup

2. Training dynamics

   - Learning Rate Schedules

     - Reduce learning rate at a few fixed points: for ResNets, multiply LR by 0.1 after epochs 30, 60, 90

     - Cosine:
       $$
       \alpha_t = \frac{1}{2}\alpha_0(1+cos(t\pi/T))
       $$

     - Linear:
       $$
       \alpha_t = \alpha_0(1-t/T)
       $$

     - Inverse sqrt:
       $$
       \alpha_t = \alpha_0/\sqrt{t}
       $$

     - Constant:
       $$
       \alpha_t = \alpha_0
       $$
       *Stop training the model when accuracy on the validation set decreases*

   - Hyperparameters optimization

     - Grid Search \ Random Search(to catch some important hyperparameters)
     - choosing LR steps
       1. Check initial loss
       2. Overfit a small sample
       3. Find LR that makes the loss drop significantly
       4. Coarse grid, train for 1-5 epochs
       5. Refine grid, train longer
       6. Look at learning curves
       7. GOTO step 
     - Track ratio of weight update / weight magnitude(around 1e-3,1e-2)

3. After training

   - Model Ensembles

     1. Train multiple independent models

     2. At test time average their results

        Tips and Tricks

        - Instead of training independent models, use multiple snapshots of a single model during training
        - Instead of using actual parameter vector, keep a moving average of the parameter vector and use that at test time

   - Transfer Learning

     1. Train on Imagenet

     2. For small dataset: Use CNN as a feature extractor(Remove the last layer)

     3. For bigger dataset: Fine-Tuning

        Continue training CNN for new task

   - Distributed Training

     - Split Model across GPUs
       - Synchronzing across GPUs is expensive
     - Copy Model on each GPU, split data
       - GPUs only communicate once per iteration, and only exchange grad params
     - Large-Batch Training
       - Scale Learning Rates
       - Learning Rate Warmup: High initial learning rates can make loss explode

## 12 Recurrent Networks

- RNN Computational Graph
  - One to One
  - One to Many![RNN-OnetoMany](D:\CSDIY\EECS498\note\RNN-OnetoMany.png)
  - Many to Many![RNN-ManytoMany](D:\CSDIY\EECS498\note\RNN-ManytoMany.png)
  - Many to One![RNN-ManytoOne](D:\CSDIY\EECS498\note\RNN-ManytoOne.png)
  - seq2seq(Many to one) + (One to many)![RNN-seq2seq](D:\CSDIY\EECS498\note\RNN-seq2seq.png)
  
- Truncated Backpropagation Through Time

- CNN+RNN![CNN+RNN](D:\CSDIY\EECS498\note\CNN+RNN.png)

- Vanilla RNN Gradient Flow

  - Multiply W for many times

    - Largest singular value > 1: Exploding gradients

      Gradient clipping

      ```python
      grad_norm = np.sum(grad * grad)
      if grad_norm > threshold:
          grad *= (threshold / grad_norm)
      ```

    - Largest singular value < 1:  Vanishing gradients

      Solution: Change RNN architecture: Long Short Term Memory(LSTM)![LSTMGradientFlow](D:\CSDIY\EECS498\note\LSTMGradientFlow.png)

      - Backpropagation from $c_t$ to $c_{t-1}$ only elementwise multiplication by f, no matrix multiply by W: Linear
        $$
        C_t = f_t*C_{t-1} + i_t*\widetilde{C}_t
        $$

## 13 Attention

- seq2seq with RNNs![seq2seqwithRNNs](D:\CSDIY\EECS498\note\seq2seqwithRNNs.png)

- seq2seq with RNNs and Attention![seq2seqwithRNNsandAttention](D:\CSDIY\EECS498\note\seq2seqwithRNNsandAttention.png)

- Image Captioning with RNNs and Attention![ImageCaptioningwithAttention](D:\CSDIY\EECS498\note\ImageCaptioningwithAttention.png)

- Attention Layer![Attention-Layer](D:\CSDIY\EECS498\note\Attention-Layer.png)

- Self-Attention Layer![Self-Attention-Layer](D:\CSDIY\EECS498\note\Self-Attention-Layer.png)

  - Self attention doesn't know the order of the vectors it is processing

    - concatenate input with positional encoding

    - Masked Self-Attention Layer![MaskedSelfAttentionLayer](D:\CSDIY\EECS498\note\MaskedSelfAttentionLayer.png)

      Don't let vectors "look ahead" in the sequence

- Multihead Self-Attention Layer

  - Use H independent "Attention Heads" in parallel
  - Hyperparameters:
    - Query dimension $D_Q$
    - Number of heads $H$

- The Transformer![TransformerBlock](D:\CSDIY\EECS498\note\TransformerBlock.png)

## 14 Visualizing and Understanding

- First Layer: Visualize Filters 

- Last Layer: 
  - Nearest Neighbor
  - Dimensionality Reduction
    - Simple algorithm: Principal Component Analysis(PCA)
    - More complex: t-SNE 
  
- Visualizing Activations

- Maximally Activating Patches

- Which Pixels Matter?

  - Saliency via Occlusion: Mask part of the image before feeding to CNN, check how much predicted probabilities change

  - Saliency via Backprop: compute gradient of (unnormalized) class score with respect to image pixels
  - Saliency Maps: Segmentation without Supervision

- Intermediate Features via (guided) backprop

- Gradient Ascent

  1. Initialize image to zeros

     Repeat:

  2. Forward image to compute current scores

  3. Backprop to get gradient of neuron value with respect to image pixels

  4. Make a small update to the image

  $$
  I = \arg\max_IS_c(I) - \lambda||I||_2^2
  $$

- DeepDream: Amplify Existing Features![DeepDream](D:\CSDIY\EECS498\note\DeepDream.png)

- Neural Sytle Transfer: Feature Inversion + Texture Synthesis

  - **Feature Inversion**![FeatureInversion](D:\CSDIY\EECS498\note\FeatureInversion.png)

    *Reconstructing from higher layers lost more features*

  - **Texture Synthesis**![TextureSynthesis](D:\CSDIY\EECS498\note\TextureSynthesis.png)

    *Reconstructing texture from higher layers recovers larger features from the input texture*

  - **Problem**: Style transfer requires many forward / backward passes through VGG; very slow

    Solution: Train another network

## 15 Object Detection

- Task Definition

  **Input**: Single RGB image

  **Output**: A set of detected objects; For each object predict:

  1. Category label (from fixed, known set of  categories)
  2. Bounding box (four numbers: x, y, width, height)

- R-CNN: Region-Based CNN![R-CNNtesttime](D:\CSDIY\EECS498\note\R-CNNtesttime.png)

  - Region proposal: $(p_x, p_y, p_h, p_w)$

  - Transform:  $(t_x, t_y, t_h, t_w)$

  - Output box: $(b_x, b_y, b_h, b_w)$

  - Translate relative to box size: 
    $$
    b_x = p_x + p_wt_x \qquad b_y = p_y + p_ht_y
    $$

  - Log-space scale transform:
    $$
    b_w = p_wexp(t_w) \qquad b_h = p_hexp(t_h)
    $$

  - Comparing Boxes: Intersection over Union (IoU)
    $$
    \frac{Area of Intersection}{Area of Union}
    $$
    *IoU > 0.5 is "decent", IoU > 0.7 is "pretty good", IoU > 0.9 is "almost perfect"*

  - Non-Max Suppression (NMS)
  
    1. Select next highest-scoring box
  
    2. Eliminate lower-scoring boxes with IoU > threshold
  
    3. If any boxes remain, GOTO 1
  
       **Problem**: NMS may eliminate "good" boxes when objects are highly overlapping
  
       no good solution
  
  - Mean Average Precision (mAP)![mAP](D:\CSDIY\EECS498\note\mAP.png)
  
    Mean Average Precision (mAP) = average of AP for each category
  
  **Problem**: R-CNN very slow, need to do 2k forward passes for each image
  
  **Solution**: Run CNN before warping
  
- Fast R-CNN![FastR-CNN](D:\CSDIY\EECS498\note\FastR-CNN.png)

  - Cropping Features: RoI Pool![RoIPool](D:\CSDIY\EECS498\note\RoIPool.png)

    **Problem**: Slight misalignment due to snapping, different-sized subregion is weird

  - **Solution**: RoI Align![RoI-Align](D:\CSDIY\EECS498\note\RoI-Align.png)

  **Problem**: Runtime dominated by region proposals

- **Solution**: Faster R-CNN: Learnable Region Proposals![FasterR-CNN](D:\CSDIY\EECS498\note\FasterR-CNN.png)

  - Insert Region Proposal Network (RPN)

    Anchor is an object? K\*20\*15

    Box transforms 4K*20\*15

  - Jointly train with 4 losses:

    1. RPN classification: anchor box is object / not an object
    2. RPN regression: predict transform from anchor box to proposal box
    3. Object classification: classify proposals as background / object class
    4. Object regression: predict transform from proposal box to object box

  Faster R-CNN is a **Two-stage object detector**

- Single-Stage Object Detection

  Classify each object as one of C categories (or background)

  - Anchor is an object? (C+1)*K\*20\*15

  - Box transforms C\*4K*20\*15 
  
- Detection without Anchors: CornerNet

## 16 Segmentation

- Semantic Segmentation

  - Label each pixel in the image with a category label
  - Don't differentiate instances, only care about pixels
  - Fully Convolutional Network![FullyConvNet](D:\CSDIY\EECS498\note\FullyConvNet.png)
    - Downsampling:
      - Pooling
      - strided
      - convolution
    - Upsampling:
      - Unpooling![Unpooling](D:\CSDIY\EECS498\note\Unpooling.png)
      - Bilinear Interpolation (also used in RoI Align)![BilinearInterpolation](D:\CSDIY\EECS498\note\BilinearInterpolation.png)
      - Bicubic Interpolation![BicubicInterpolation](D:\CSDIY\EECS498\note\BicubicInterpolation.png)
      - Max Unpooling![MaxUnpooling](D:\CSDIY\EECS498\note\MaxUnpooling.png)
      - Learnable Upsampling: Transposed Convolution![TransposedConv](D:\CSDIY\EECS498\note\TransposedConv.png)

- Instance Segmentation: Detect all objects in the image, and identify the pixels that belong to each object (Only things)

  Approach: Perform object detection, then predict a segmentation mask for each object

  - Mask R-CNN![MaskR-CNN](D:\CSDIY\EECS498\note\MaskR-CNN.png)

- Some other tasks

  - Panoptic Segmentation
  - Pose Estimation
  - Dense Captioning
